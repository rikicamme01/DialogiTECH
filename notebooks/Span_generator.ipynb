{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbVZweEEW1eD"
      },
      "source": [
        "# Span generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Mhe88vKvVjN5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('../data/Original_csv/Hyperion.csv', na_filter=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "N1jTvA17WUsV"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "dataset = []\n",
        "sample = {}\n",
        "\n",
        "for row in df.itertuples():\n",
        "    text = row.Testo\n",
        "    if text and len(text) > 4:\n",
        "        dataset.append(sample)\n",
        "        sample = {}\n",
        "        sample['Testo'] = text\n",
        "        sample['Stralci'] = [row.Stralcio]\n",
        "        sample['Repertori'] = [row.Repertorio]\n",
        "        \n",
        "    else:\n",
        "        sample['Stralci'].append(row.Stralcio)\n",
        "        sample['Repertori'].append(row.Repertorio)\n",
        "del dataset[0]\n",
        "\n",
        "#Find bounds starting froma text\n",
        "def find_char_bounds(spans: list, text: str) -> list:\n",
        "    '''\n",
        "    Given a list of spans and a text, find the start and end indices of each span in the text.\n",
        "    Indeces are computed counting chars\n",
        "    \n",
        "    :param spans: a list of strings to search for\n",
        "    :type spans: list\n",
        "    :param text: the text to search\n",
        "    :type text: str\n",
        "    :return: A list of tuples, where each tuple contains the start and end index of a span.\n",
        "    '''\n",
        "    bounds = []\n",
        "    last_char = 0\n",
        "    for span in spans:\n",
        "        start = text.find(span)\n",
        "        if start == -1:\n",
        "            start = last_char + 1\n",
        "        bounds.append((start, start + len(span)))\n",
        "        last_char = start + len(span)\n",
        "    return bounds\n",
        "\n",
        "'''\n",
        "Given a list of spans and a text, find the start and end indices of each span in the text.\n",
        "Indeces are computed counting WORDS.\n",
        "\n",
        ":param spans: a list of strings, each string is a span of text\n",
        ":type spans: list\n",
        ":param text: the text to be searched\n",
        ":type text: str\n",
        ":return: A list of tuples, where each tuple is the start and end index of a word in the text.\n",
        "'''\n",
        "def find_word_bounds(spans: list, text: str) -> list:\n",
        "    \n",
        "    bounds = []\n",
        "    end = 0\n",
        "    for span in spans:\n",
        "        s = span.translate(str.maketrans('', '', string.punctuation))\n",
        "        word_list = s.split()\n",
        "        text_list = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
        "        try:\n",
        "            start = text_list.index(word_list[0], end)\n",
        "        except:\n",
        "            if not bounds:\n",
        "                start = 0\n",
        "            else:\n",
        "                \n",
        "                start = bounds[-1][1] + 1\n",
        "        end = start + len(word_list) - 1\n",
        "            \n",
        "        bounds.append((start, end))\n",
        "    return bounds\n",
        "\n",
        "\n",
        "for sample in dataset:\n",
        "    #sample['Bounds'] = find_char_bounds(sample['Stralci'], sample['Testo'])\n",
        "    sample['Bounds'] = find_word_bounds(sample['Stralci'], sample['Testo'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numero testi nel dataset 15572\n",
            "Numero span nel dataset: 35471\n",
            "Media span per testo:  2.2778705368610326\n",
            "Massimo numero di span in un singolo testo:  94\n"
          ]
        }
      ],
      "source": [
        "print('Numero testi nel dataset', str(len(dataset)))\n",
        "n_spans = 0\n",
        "for e in dataset:\n",
        "    n_spans += len(e['Bounds'])\n",
        "print('Numero span nel dataset:', str(n_spans))\n",
        "print('Media span per testo: ', str(n_spans / len(dataset)))\n",
        "max = 0\n",
        "max_i = 0\n",
        "for i,e in enumerate(dataset):\n",
        "    if len(e['Bounds']) > max:\n",
        "        max = len(e['Bounds'])\n",
        "        max_i = i\n",
        "print('Massimo numero di span in un singolo testo: ', str(max))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUofuB4KWhf3",
        "outputId": "95a850fb-aa28-437f-b07f-d256f2fb399e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/michele/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize\n",
        "import re\n",
        "\n",
        "nltk_pred = []\n",
        "spans_pred = []\n",
        "\n",
        "for sample in dataset:\n",
        "    tokens = sent_tokenize(sample['Testo'])\n",
        "    spans = []\n",
        "    bounds = []\n",
        "    \"\"\"\n",
        "    for x in tokens:\n",
        "        #spans += re.findall('.*?[.:!?;,]', x)\n",
        "        spans += re.split('[]', x)\n",
        "        spans = list(filter(None, spans)) # filter empty strings\n",
        "    \"\"\"\n",
        "    #bounds += find_char_bounds(spans, sample['Testo'])\n",
        "    bounds += find_word_bounds(tokens, sample['Testo'])\n",
        "    nltk_pred.append(bounds)\n",
        "    spans_pred.append(tokens) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "r1N5H04_WlbH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# A è B sono tupe con i bound dello span\n",
        "def IoU(A, B):\n",
        "    if A == B:\n",
        "        return 1\n",
        "    start = max(A[0], B[0])\n",
        "    end = min(A[1], B[1])\n",
        "    if(start > end):\n",
        "        return 0\n",
        "    intersection = end - start\n",
        "    return intersection / (A[1] - A[0] + B[1] - B[0] - intersection)\n",
        "\n",
        "def compute_IoUs(pred_bounds, gt_spans):\n",
        "    IoUs = []\n",
        "    for gt_bounds in gt_spans:\n",
        "        IoUs.append(IoU(pred_bounds, gt_bounds)) \n",
        "    return IoUs\n",
        "\n",
        "#Input: text_spans_dict = [ {\n",
        "#           'Bounds' : (a,b), \n",
        "#           'IoU' : float,\n",
        "#           'Repertorio': 'string':\n",
        "#           } ]\n",
        "def normalize(text_spans_dict, gt_spans):\n",
        "    normalized = []\n",
        "    for i in range(len(text_spans_dict)):\n",
        "        #normalized is not empty\n",
        "        if normalized:\n",
        "            if normalized[-1]['Repertorio'] == text_spans_dict[i]['Repertorio']:\n",
        "                new_span = (normalized[-1]['Bounds'][0], text_spans_dict[i]['Bounds'][1])\n",
        "                new_span_features = {\n",
        "                    'Bounds' : new_span, \n",
        "                    'IoU' : None,\n",
        "                    'Repertorio' : text_spans_dict[i]['Repertorio']\n",
        "                    }\n",
        "                del normalized[-1]\n",
        "                normalized.append(new_span_features)\n",
        "            else:\n",
        "                normalized.append(text_spans_dict[i])\n",
        "        else:\n",
        "            normalized.append(text_spans_dict[i])\n",
        "        \n",
        "    \n",
        "    for i in range(len(normalized)):\n",
        "        normalized[i]['IoU'] = max(compute_IoUs(normalized[i]['Bounds'], gt_spans['Bounds']))\n",
        "    return normalized\n",
        "    \n",
        "\n",
        "metrics = []\n",
        "normalized_metrics = []\n",
        "for i, pred_bounds in enumerate(nltk_pred):\n",
        "    text_IoUs = []\n",
        "    for pred_span in pred_bounds:\n",
        "        IoUs = compute_IoUs(pred_span, dataset[i]['Bounds'])\n",
        "        best = np.argmax(IoUs)\n",
        "        span_features = {\n",
        "            'Bounds' : pred_span, \n",
        "            'IoU' : IoUs[best],\n",
        "            'Repertorio' : dataset[i]['Repertori'][best]\n",
        "            }\n",
        "\n",
        "        text_IoUs.append(span_features)\n",
        "    metrics.append(text_IoUs)\n",
        "    normalized_metrics.append(normalize(text_IoUs, dataset[i]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "Risultati labels GT e stralci non uniti\n",
            "Numero stralci nel dataset: 35471\n",
            "Numero stralci predetti: 64447\n",
            "Numero stralci con lunghezza minima =  0 :  63200\n",
            "Media IoU: 0.3731696813940625\n",
            "Percentuale span perfetti:  0.1818987341772152\n"
          ]
        }
      ],
      "source": [
        "print('----------------------------------------------------------')\n",
        "print('Risultati labels GT e stralci non uniti')\n",
        "\n",
        "\n",
        "n_spans = 0\n",
        "for e in dataset:\n",
        "    n_spans += len(e['Bounds'])\n",
        "print('Numero stralci nel dataset:', str(n_spans))\n",
        "\n",
        "n_spans = 0\n",
        "for e in metrics:\n",
        "    n_spans += len(e)\n",
        "print('Numero stralci predetti:', str(n_spans))\n",
        "\n",
        "mean = 0\n",
        "long_spans = 0\n",
        "min_lenght = 0\n",
        "perfect_spans =0\n",
        "for text in metrics:\n",
        "    for span in text:\n",
        "        if span['Bounds'][1] - span['Bounds'][0] >= min_lenght:\n",
        "            long_spans += 1\n",
        "            mean += span['IoU']\n",
        "            if span['IoU'] == 1:\n",
        "                perfect_spans += 1\n",
        "perfect_spans_perc = perfect_spans / long_spans\n",
        "mean_IoU = mean / long_spans\n",
        "print('Numero stralci con lunghezza minima = ', str(min_lenght), ': ', str(long_spans))\n",
        "print('Media IoU:', str(mean_IoU))\n",
        "print('Percentuale span perfetti: ', str(perfect_spans_perc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBBqubrGWpiv",
        "outputId": "e4aedf5d-7942-4329-9f20-1dcc36da9d0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "Risultati labels GT e stralci uniti\n",
            "Numero stralci nel dataset: 35471\n",
            "Numero stralci predetti: 26781\n",
            "Numero stralci con lunghezza minima =  0 :  26469\n",
            "Media IoU: 0.8822384163157678\n",
            "Percentuale span perfetti:  0.7178208470285995\n"
          ]
        }
      ],
      "source": [
        "print('----------------------------------------------------------')\n",
        "print('Risultati labels GT e stralci uniti')\n",
        "\n",
        "\n",
        "n_spans = 0\n",
        "for e in dataset:\n",
        "    n_spans += len(e['Bounds'])\n",
        "print('Numero stralci nel dataset:', str(n_spans))\n",
        "\n",
        "n_spans = 0\n",
        "for e in normalized_metrics:\n",
        "    n_spans += len(e)\n",
        "print('Numero stralci predetti:', str(n_spans))\n",
        "\n",
        "mean = 0\n",
        "long_spans = 0\n",
        "min_lenght = 0\n",
        "perfect_spans =0\n",
        "for text in normalized_metrics:\n",
        "    for span in text:\n",
        "        if span['Bounds'][1] - span['Bounds'][0] >= min_lenght:\n",
        "            long_spans += 1\n",
        "            mean += span['IoU']\n",
        "            if span['IoU'] == 1:\n",
        "                perfect_spans += 1\n",
        "perfect_spans_perc = perfect_spans / long_spans\n",
        "mean_IoU = mean / long_spans\n",
        "print('Numero stralci con lunghezza minima = ', str(min_lenght), ': ', str(long_spans))\n",
        "print('Media IoU:', str(mean_IoU))\n",
        "print('Percentuale span perfetti: ', str(perfect_spans_perc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MI0-t_EWuaO"
      },
      "source": [
        "# Span classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqaKAD_5XLro",
        "outputId": "097feac4-1748-44f6-9d1f-13ba15b71084"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/michele/anaconda3/lib/python3.9/site-packages (4.12.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/michele/anaconda3/lib/python3.9/site-packages (from transformers) (1.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/michele/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/michele/anaconda3/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/michele/anaconda3/lib/python3.9/site-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/michele/anaconda3/lib/python3.9/site-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/michele/anaconda3/lib/python3.9/site-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /home/michele/anaconda3/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
            "Requirement already satisfied: sacremoses in /home/michele/anaconda3/lib/python3.9/site-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/michele/anaconda3/lib/python3.9/site-packages (from transformers) (2021.8.3)\n",
            "Requirement already satisfied: filelock in /home/michele/anaconda3/lib/python3.9/site-packages (from transformers) (3.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/michele/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /home/michele/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/michele/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/michele/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/michele/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/michele/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: click in /home/michele/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.3)\n",
            "Requirement already satisfied: joblib in /home/michele/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /home/michele/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "XwKUZkrBWydE"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('MiBo/RepML')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFjoRPxdXZ1a",
        "outputId": "f5cfd1bb-63c8-449c-ec1d-67765c189645"
      },
      "outputs": [],
      "source": [
        "predicted_dataset = []\n",
        "\n",
        "\n",
        "for i, span_group in enumerate(spans_pred):\n",
        "  text_features = {}\n",
        "  text_features['Testo'] = dataset[i]['Testo']\n",
        "  text_features['Stralci'] = [span.lower() for span in span_group]\n",
        "  text_features['Bounds'] = nltk_pred[i]\n",
        "  predicted_dataset.append(text_features)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numero testi nel dataset 15572\n",
            "Numero span nel dataset: 64447\n",
            "Media span per testo:  4.138646288209607\n",
            "Massimo numero di span in un singolo testo:  126\n"
          ]
        }
      ],
      "source": [
        "print('Numero testi nel dataset', str(len(dataset)))\n",
        "n_spans = 0\n",
        "for e in predicted_dataset:\n",
        "    n_spans += len(e['Bounds'])\n",
        "print('Numero span nel dataset:', str(n_spans))\n",
        "print('Media span per testo: ', str(n_spans / len(predicted_dataset)))\n",
        "max = 0\n",
        "max_i = 0\n",
        "for i,e in enumerate(predicted_dataset):\n",
        "    if len(e['Bounds']) > max:\n",
        "        max = len(e['Bounds'])\n",
        "        max_i = i\n",
        "print('Massimo numero di span in un singolo testo: ', str(max))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.588042640637041"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "55873 / 15572"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "oLXXkBsRoh8I"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn import preprocessing\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "LABELS = [\n",
        "                'anticipazione',\n",
        "                'causa',\n",
        "                'commento',\n",
        "                'conferma',\n",
        "                'considerazione',\n",
        "                'contrapposizione',\n",
        "                'deresponsabilizzazione',\n",
        "                'descrizione',\n",
        "                'dichiarazione di intenti',\n",
        "                'generalizzazione',\n",
        "                'giudizio',\n",
        "                'giustificazione',\n",
        "                'implicazione',\n",
        "                'non risposta',\n",
        "                'opinione',\n",
        "                'possibilità',\n",
        "                'prescrizione',\n",
        "                'previsione',\n",
        "                'proposta',\n",
        "                'ridimensionamento',\n",
        "                'sancire',\n",
        "                'specificazione',\n",
        "                'valutazione'\n",
        "        ]\n",
        "\n",
        "\n",
        "def decode_labels(encoded_labels):\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    le.fit(LABELS)\n",
        "    return le.inverse_transform(encoded_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "04313pMw4p4Y"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import utils\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def predict_labels(text: dict)-> list:\n",
        "  pred = []\n",
        "  if text['Stralci']:\n",
        "    encodings = tokenizer(\n",
        "          text['Stralci'],\n",
        "          max_length=512,\n",
        "          add_special_tokens=True,\n",
        "          return_attention_mask=True,\n",
        "          padding=True,\n",
        "          truncation=True,\n",
        "          return_tensors=\"pt\"\n",
        "      )\n",
        "      \n",
        "  test_dataset = TensorDataset(encodings['input_ids'],encodings['attention_mask'])\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        " \n",
        "  for i, batch in enumerate(test_dataloader):\n",
        "      b_input_ids = batch[0].to(device)\n",
        "      b_input_mask = batch[1].to(device)\n",
        "      with torch.no_grad():        \n",
        "\n",
        "          # Forward pass, calculate logits\n",
        "          # argmax(logits) = argmax(Softmax(logits))\n",
        "          outputs = model(b_input_ids, \n",
        "                                  attention_mask=b_input_mask)\n",
        "          logits = outputs[0]\n",
        "\n",
        "      logits = logits.detach().cpu()\n",
        "\n",
        "      batch_pred = logits.softmax(dim=1)\n",
        "      pred += batch_pred.argmax(dim=1)\n",
        "  return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "vPf_u-IK73wQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "testo:  0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_160369/3555640013.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mpredicted_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Repertori'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_160369/513660088.py\u001b[0m in \u001b[0;36mpredict_labels\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0;31m# Forward pass, calculate logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m           \u001b[0;31m# argmax(logits) = argmax(Softmax(logits))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m           outputs = model(b_input_ids, \n\u001b[0m\u001b[1;32m     31\u001b[0m                                   attention_mask=b_input_mask)\n\u001b[1;32m     32\u001b[0m           \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1528\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1531\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         )\n\u001b[0;32m--> 996\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    997\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    581\u001b[0m                 )\n\u001b[1;32m    582\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    584\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     ):\n\u001b[0;32m--> 400\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mtranspose_for_scores\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mnew_x_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_x_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Setup for testing with gpu\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "for i, text in enumerate(predicted_dataset):\n",
        "  pred = predict_labels(text)\n",
        "  rep = decode_labels(list(pred))\n",
        "  predicted_dataset[i]['Repertori'] =rep\n",
        "\n",
        "  if i%100==0:\n",
        "    print('testo: ', str(i))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "cHXeeghItQUW",
        "outputId": "b38040ee-de4d-481a-d2a9-f50431a6a27f"
      },
      "outputs": [],
      "source": [
        "metrics = []\n",
        "normalized_metrics = []\n",
        "for i, sample in enumerate(predicted_dataset):\n",
        "    text_IoUs = []\n",
        "    for j, pred_bounds in enumerate(sample['Bounds']):\n",
        "        IoUs = compute_IoUs(pred_bounds, dataset[i]['Bounds'])\n",
        "        best = np.argmax(IoUs)\n",
        "        span_features = {\n",
        "            'Bounds' : pred_bounds, \n",
        "            'IoU' : IoUs[best],\n",
        "            'Repertorio' : predicted_dataset[i]['Repertori'][j]\n",
        "            }\n",
        "\n",
        "        text_IoUs.append(span_features)\n",
        "    metrics.append(text_IoUs)\n",
        "    normalized_metrics.append(normalize(text_IoUs, dataset[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "4HWRrvD-wMUg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numero stralci nel dataset: 35471\n",
            "Numero stralci predetti: 151228\n",
            "Numero stralci con lunghezza minima =  0 :  150997\n",
            "Media IoU: 0.13439584113870004\n",
            "Percentuale span perfetti:  0.0458618383146685\n"
          ]
        }
      ],
      "source": [
        "print('----------------------------------------------------------')\n",
        "print('Risultati labels predette e stralci NON uniti')\n",
        "\n",
        "\n",
        "n_spans = 0\n",
        "for e in dataset:\n",
        "    n_spans += len(e['Bounds'])\n",
        "print('Numero stralci nel dataset:', str(n_spans))\n",
        "\n",
        "n_spans = 0\n",
        "for e in metrics:\n",
        "    n_spans += len(e)\n",
        "print('Numero stralci predetti:', str(n_spans))\n",
        "\n",
        "mean = 0\n",
        "long_spans = 0\n",
        "min_lenght = 0\n",
        "perfect_spans =0\n",
        "for text in metrics:\n",
        "    for span in text:\n",
        "        if span['Bounds'][1] - span['Bounds'][0] >= min_lenght:\n",
        "            long_spans += 1\n",
        "            mean += span['IoU']\n",
        "            if span['IoU'] == 1:\n",
        "                perfect_spans += 1\n",
        "perfect_spans_perc = perfect_spans / long_spans\n",
        "mean_IoU = mean / long_spans\n",
        "print('Numero stralci con lunghezza minima = ', str(min_lenght), ': ', str(long_spans))\n",
        "print('Media IoU:', str(mean_IoU))\n",
        "print('Percentuale span perfetti: ', str(perfect_spans_perc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('----------------------------------------------------------')\n",
        "print('Risultati labels predette e stralci uniti')\n",
        "\n",
        "n_spans = 0\n",
        "for e in dataset:\n",
        "    n_spans += len(e['Bounds'])\n",
        "print('Numero stralci nel dataset:', str(n_spans))\n",
        "\n",
        "n_spans = 0\n",
        "for e in normalized_metrics:\n",
        "    n_spans += len(e)\n",
        "print('Numero stralci predetti:', str(n_spans))\n",
        "\n",
        "mean = 0\n",
        "long_spans = 0\n",
        "min_lenght = 0\n",
        "perfect_spans =0\n",
        "for text in normalized_metrics:\n",
        "    for span in text:\n",
        "        if span['Bounds'][1] - span['Bounds'][0] >= min_lenght:\n",
        "            long_spans += 1\n",
        "            mean += span['IoU']\n",
        "            if span['IoU'] == 1:\n",
        "                perfect_spans += 1\n",
        "perfect_spans_perc = perfect_spans / long_spans\n",
        "mean_IoU = mean / long_spans\n",
        "print('Numero stralci con lunghezza minima = ', str(min_lenght), ': ', str(long_spans))\n",
        "print('Media IoU:', str(mean_IoU))\n",
        "print('Percentuale span perfetti: ', str(perfect_spans_perc))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Span_generator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
