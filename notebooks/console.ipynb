{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "LABELS = [\n",
    "            'anticipazione',\n",
    "            'causa',\n",
    "            'commento',\n",
    "            'conferma',\n",
    "            'considerazione',\n",
    "            'contrapposizione',\n",
    "            'deresponsabilizzazione',\n",
    "            'descrizione',\n",
    "            'dichiarazione di intenti',\n",
    "            'generalizzazione',\n",
    "            'giudizio',\n",
    "            'giustificazione',\n",
    "            'implicazione',\n",
    "            'non risposta',\n",
    "            'opinione',\n",
    "            'possibilità',\n",
    "            'prescrizione',\n",
    "            'previsione',\n",
    "            'proposta',\n",
    "            'ridimensionamento',\n",
    "            'sancire',\n",
    "            'specificazione',\n",
    "            'valutazione'\n",
    "    ]\n",
    "\n",
    "class HyperionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer_name):\n",
    "        #fill_null_features(df)\n",
    "        df = filter_empty_labels(df)\n",
    "        df = to_lower_case(df)\n",
    "        uniform_labels(df)          \n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name) \n",
    "        self.encodings = tokenize(df, tokenizer)\n",
    "        self.labels = encode_labels(df).tolist()    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        \n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)   \n",
    "\n",
    "\n",
    "# Dataset loading and preprocessing\n",
    "def fill_null_features(df):\n",
    "        for c in ['Domanda','Testo']:\n",
    "            for i in range(0,len(df.index)):  \n",
    "                if not df[c][i]:\n",
    "                    j=i\n",
    "                    while j>0: \n",
    "                        j-=1\n",
    "                        if df[c][j]:\n",
    "                            df[c][i] = df[c][j]\n",
    "                            break\n",
    "\n",
    "#Delete examples with empty label\n",
    "def filter_empty_labels(df):\n",
    "    filter = df[\"Repertorio\"] != \"\"\n",
    "    return df[filter]\n",
    "\n",
    "#Convert to lower case\n",
    "def to_lower_case(df):\n",
    "    return df.applymap(str.lower)\n",
    "\n",
    "\n",
    "#Lables uniformation uncased\n",
    "def uniform_labels(df):\n",
    "    df['Repertorio'].replace('implicazioni','implicazione', inplace=True)\n",
    "    df['Repertorio'].replace('previsioni','previsione', inplace=True)\n",
    "\n",
    "def tokenize(df, tokenizer):\n",
    "    return tokenizer(\n",
    "        df['Stralcio'].tolist(),\n",
    "        #df['Domanda'].tolist(),\n",
    "        max_length=512,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "def encode_labels(df):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(LABELS)\n",
    "    return le.transform(df['Repertorio'])\n",
    "\n",
    "def decode_labels(encoded_labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(LABELS)\n",
    "    return le.inverse_transform(encoded_labels)\n",
    "\n",
    "def train_val_split(df, tok_name,  val_perc=0.1):\n",
    "    gb = df.groupby('Repertorio')\n",
    "    train_list = []\n",
    "    val_list = []\n",
    "\n",
    "    for x in gb.groups:\n",
    "        class_df = gb.get_group(x)\n",
    "\n",
    "        # Validation set creation\n",
    "        val = class_df.sample(frac=val_perc)\n",
    "        train = pd.concat([class_df,val]).drop_duplicates(keep=False)\n",
    "\n",
    "        #train_list.append(train.head(500))\n",
    "        train_list.append(train)\n",
    "        val_list.append(val)\n",
    "\n",
    "    train_df = pd.concat(train_list)\n",
    "    val_df = pd.concat(val_list)\n",
    "    return HyperionDataset(train_df, tok_name), HyperionDataset(val_df, tok_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Original_csv/Hyperion.csv', na_filter=False)\n",
    "tok_name = \"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\"\n",
    "\n",
    "train_dataset, val_dataset = train_val_split(df.head(100), tok_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deterministic mode\n",
    "def seed_everything(seed=1464):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "class NeptuneLogger():\n",
    "    def __init__(self) -> None:\n",
    "        #Neptune initialization\n",
    "        self.run = neptune.init(\n",
    "            project=\"mibo8/Rep\",\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmZmRkYThiZi1mZGNlLTRlMTktODQwNS1hNWFlMWQ2Mjc4N2IifQ==\",\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "import torchmetrics\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import  AdamW\n",
    "\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, batch_size, lr, n_epochs, ) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = lr\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        self.logger = NeptuneLogger()\n",
    "\n",
    "        self.metric_collection = torchmetrics.MetricCollection({\n",
    "\n",
    "            'accuracy_micro' : torchmetrics.Accuracy(num_classes=23, multiclass=True, average='micro'),\n",
    "            'accuracy_macro' : torchmetrics.Accuracy(num_classes=23, multiclass=True, average='macro'),\n",
    "            'accuracy_weighted' : torchmetrics.Accuracy(num_classes=23, multiclass=True, average='weighted'),\n",
    "            'accuracy_none' : torchmetrics.Accuracy(num_classes=23, multiclass=True, average='none'),\n",
    "\n",
    "            'f1_micro' : torchmetrics.F1(num_classes=23, multiclass=True, average='micro'),\n",
    "            'f1_macro' : torchmetrics.F1(num_classes=23, multiclass=True, average='macro'),\n",
    "            'f1_weighted' : torchmetrics.F1(num_classes=23, multiclass=True, average='weighted'),\n",
    "            'f1_none' : torchmetrics.F1(num_classes=23, multiclass=True, average='none'),\n",
    "\n",
    "            'precision_micro' : torchmetrics.Precision(num_classes=23, multiclass=True, average='micro'),\n",
    "            'precision_macro' : torchmetrics.Precision(num_classes=23, multiclass=True, average='macro'),\n",
    "            'precision_weighted' : torchmetrics.Precision(num_classes=23, multiclass=True, average='weighted'),\n",
    "            'precision_none' : torchmetrics.Precision(num_classes=23, multiclass=True, average='none'),\n",
    "\n",
    "            'recall_micro' : torchmetrics.Recall(num_classes=23, multiclass=True, average='micro'),\n",
    "            'recall_macro' : torchmetrics.Recall(num_classes=23, multiclass=True, average='macro'),\n",
    "            'recall_weighted' : torchmetrics.Recall(num_classes=23, multiclass=True, average='weighted'),\n",
    "            'recall_none' : torchmetrics.Recall(num_classes=23, multiclass=True, average='none')\n",
    "        }) \n",
    "        \n",
    "    def fit(self, model, train_dataset, val_dataset):\n",
    "        \n",
    "        params_info = {\n",
    "            'learning_rate' : self.learning_rate,\n",
    "            'batch_size' : self.batch_size,\n",
    "            'n_epochs' : self.n_epochs\n",
    "        }\n",
    "        self.logger.run['params'] = params_info\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        #----------TRAINING\n",
    "\n",
    "        # Measure the total training time for the whole run.\n",
    "        total_t0 = time.time()\n",
    "\n",
    "        epochs = self.n_epochs\n",
    "\n",
    "        # Creation of Pytorch DataLoaders with shuffle=True for the traing phase\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        validation_dataloader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        #Adam algorithm optimized for tranfor architectures\n",
    "        optimizer = AdamW(model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Scaler for mixed precision\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        # Setup for training with gpu\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        model.to(device)\n",
    "\n",
    "        # For each epoch...\n",
    "        for epoch_i in range(0, epochs):\n",
    "            \n",
    "            # ========================================\n",
    "            #               Training\n",
    "            # ========================================\n",
    "            \n",
    "            # Perform one full pass over the training set.\n",
    "\n",
    "            print(\"\")\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            print('Training...')\n",
    "\n",
    "            # Measure how long the training epoch takes.\n",
    "            t0 = time.time()\n",
    "\n",
    "            # Reset the total loss for this epoch.\n",
    "            total_train_loss = 0\n",
    "\n",
    "            # Put the model into training mode: Dropout layers are active\n",
    "            model.train()\n",
    "            \n",
    "            # For each batch of training data...\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "                # Progress update every 40 batches.\n",
    "                if step % 10 == 0 and not step == 0:\n",
    "                    # Compute time in minutes.\n",
    "                    elapsed = format_time(time.time() - t0)\n",
    "                    \n",
    "                    # Report progress.\n",
    "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "                # Unpack this training batch from the dataloader. \n",
    "                #\n",
    "                #  copy each tensor to the GPU using the 'to()' method\n",
    "                #\n",
    "                # 'batch' contains three pytorch tensors:\n",
    "                #   [0]: input ids \n",
    "                #   [1]: attention masks\n",
    "                #   [2]: labels \n",
    "                b_input_ids = batch['input_ids'].to(device)\n",
    "                b_input_mask = batch['attention_mask'].to(device)\n",
    "                b_labels = batch['labels'].to(device)\n",
    "\n",
    "                # clear any previously calculated gradients before performing a\n",
    "                # backward pass\n",
    "                model.zero_grad()  \n",
    "\n",
    "\n",
    "                # Perform a forward pass in mixed precision\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(b_input_ids, \n",
    "                                    attention_mask=b_input_mask, \n",
    "                                    labels=b_labels)\n",
    "                \n",
    "                loss = outputs[0]\n",
    "                logits = outputs[1]\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                logits = logits.detach().cpu()\n",
    "                label_ids = b_labels.to('cpu')\n",
    "\n",
    "                batch_metric = self.metric_collection.update(logits, label_ids)\n",
    "                #print(batch_metric)\n",
    "\n",
    "                # Perform a backward pass to compute the gradients in MIXED precision\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Accumulate the training loss over all of the batches so that we can\n",
    "                # calculate the average loss at the end.\n",
    "                total_train_loss += loss.item()\n",
    "\n",
    "                # Unscales the gradients of optimizer's assigned params in-place before the gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "\n",
    "                # Clip the norm of the gradients to 1.0.\n",
    "                # This helps and prevent the \"exploding gradients\" problem.\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                # Update parameters and take a step using the computed gradient in MIXED precision\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "\n",
    "            # Compute the average loss over all of the batches.\n",
    "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "            final_metrics = self.metric_collection.compute()\n",
    "            print(final_metrics)\n",
    "            \n",
    "            # Measure how long this epoch took.\n",
    "            training_time = format_time(time.time() - t0)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "            print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "            # ========================================\n",
    "            #               Validation\n",
    "            # ========================================\n",
    "            # After the completion of each training epoch, measure performance on\n",
    "            # the validation set.\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\")\n",
    "\n",
    "            self.metric_collection.reset()\n",
    "            t0 = time.time()\n",
    "\n",
    "            # Put the model in evaluation mode: the dropout layers behave differently\n",
    "            model.eval()\n",
    "\n",
    "            total_val_loss = 0\n",
    "\n",
    "            # Evaluate data for one epoch\n",
    "            for batch in validation_dataloader:\n",
    "                \n",
    "                # Unpack this training batch from our dataloader. \n",
    "                #\n",
    "                # copy each tensor to the GPU using the 'to()' method\n",
    "                #\n",
    "                # 'batch' contains three pytorch tensors:\n",
    "                #   [0]: input ids \n",
    "                #   [1]: attention masks\n",
    "                #   [2]: labels \n",
    "                b_input_ids = batch['input_ids'].to(device)\n",
    "                b_input_mask = batch['attention_mask'].to(device)\n",
    "                b_labels = batch['labels'].to(device)\n",
    "                \n",
    "                # Tell pytorch not to bother with constructing the compute graph during\n",
    "                # the forward pass, since this is only needed for training.\n",
    "                with torch.no_grad():        \n",
    "\n",
    "                    # Forward pass, calculate logits\n",
    "                    # argmax(logits) = argmax(Softmax(logits))\n",
    "                    outputs = model(b_input_ids, \n",
    "                                        attention_mask=b_input_mask,\n",
    "                                        labels=b_labels)\n",
    "                    loss = outputs[0]\n",
    "                    logits = outputs[1]\n",
    "                    \n",
    "                # Accumulate the validation loss.\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                logits = logits.detach().cpu()\n",
    "                label_ids = b_labels.to('cpu')\n",
    "\n",
    "                # metric on current batch\n",
    "                batch_metric = self.metric_collection.update(logits.softmax(dim=1), label_ids)\n",
    "\n",
    "            # Report the final metrics for this validation phase.\n",
    "            # metric on all batches using custom accumulation from torchmetrics library\n",
    "\n",
    "            final_metrics = self.metric_collection.compute()\n",
    "            print('VALIDATION: ')\n",
    "            print(final_metrics)\n",
    "            # Compute the average loss over all of the batches.\n",
    "            avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "            \n",
    "            # Measure how long the validation run took.\n",
    "            validation_time = format_time(time.time() - t0)\n",
    "            \n",
    "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "            print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "        print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "https://app.neptune.ai/mibo8/Rep/e/REP-98\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(2, 1e-5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_string = \"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_string)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_string, num_labels=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michele/anaconda3/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "/tmp/ipykernel_19040/2032609219.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/michele/anaconda3/lib/python3.9/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch    10  of     47.    Elapsed: 0:00:18.\n",
      "  Batch    20  of     47.    Elapsed: 0:00:36.\n",
      "  Batch    30  of     47.    Elapsed: 0:00:54.\n",
      "  Batch    40  of     47.    Elapsed: 0:01:11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n",
      "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_macro': tensor(0.0774), 'accuracy_micro': tensor(0.1489), 'accuracy_none': tensor([   nan, 0.0000, 0.6250, 0.7500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2500, 0.0000,\n",
      "           nan, 0.0000, 0.0000, 0.0000, 0.0000]), 'accuracy_weighted': tensor(0.1489), 'f1_macro': tensor(0.0376), 'f1_micro': tensor(0.1489), 'f1_none': tensor([   nan, 0.0000, 0.3390, 0.1176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000,\n",
      "           nan, 0.0000, 0.0000, 0.0000, 0.0000]), 'f1_weighted': tensor(0.0769), 'precision_macro': tensor(0.0379), 'precision_micro': tensor(0.1489), 'precision_none': tensor([   nan, 0.0000, 0.2326, 0.0638, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000,\n",
      "           nan, 0.0000, 0.0000, 0.0000, 0.0000]), 'precision_weighted': tensor(0.0636), 'recall_macro': tensor(0.0774), 'recall_micro': tensor(0.1489), 'recall_none': tensor([   nan, 0.0000, 0.6250, 0.7500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2500, 0.0000,\n",
      "           nan, 0.0000, 0.0000, 0.0000, 0.0000]), 'recall_weighted': tensor(0.1489)}\n",
      "\n",
      "  Average training loss: 3.070\n",
      "  Training epoch took: 0:01:24\n",
      "\n",
      "Running Validation...\n",
      "VALIDATION: \n",
      "{'accuracy_macro': tensor(0.2000), 'accuracy_micro': tensor(0.3333), 'accuracy_none': tensor([nan, 0., 1., nan, nan, nan, nan, nan, nan, nan, 0., nan, nan, 0., nan, nan, nan, nan,\n",
      "        nan, nan, 0., nan, nan]), 'accuracy_weighted': tensor(0.3333), 'f1_macro': tensor(0.1000), 'f1_micro': tensor(0.3333), 'f1_none': tensor([   nan, 0.0000, 0.5000,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan, 0.0000,    nan,    nan, 0.0000,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan, 0.0000,    nan,    nan]), 'f1_weighted': tensor(0.1667), 'precision_macro': tensor(0.0667), 'precision_micro': tensor(0.3333), 'precision_none': tensor([   nan, 0.0000, 0.3333,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan, 0.0000,    nan,    nan, 0.0000,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan, 0.0000,    nan,    nan]), 'precision_weighted': tensor(0.1111), 'recall_macro': tensor(0.2000), 'recall_micro': tensor(0.3333), 'recall_none': tensor([nan, 0., 1., nan, nan, nan, nan, nan, nan, nan, 0., nan, nan, 0., nan, nan, nan, nan,\n",
      "        nan, nan, 0., nan, nan]), 'recall_weighted': tensor(0.3333)}\n",
      "  Validation Loss: 2.37\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:01:24 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model,train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(history):\n",
    "  plt.figure(figsize=(10,6))\n",
    "  plt.plot(history.epoch,history.history['loss'])\n",
    "  plt.plot(history.epoch,history.history['val_loss'])\n",
    "  plt.title('loss')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
